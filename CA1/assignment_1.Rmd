---
title: 'ECMM450 Stochastic Processes: Assignment 1'
author: '700054986'
date: "`r Sys.Date()`"
output:
  pdf_document: default
  html_document: default
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
knitr::opts_chunk$set(fig.width=4, fig.height=4)
knitr::opts_chunk$set(fig.align='center')
knitr::opts_chunk$set(tidy.opts=list(width.cutoff=60),tidy=TRUE)
options(warn=-1)
```

**1** Consider the discrete random variable X that has the probability generating function

\begin{center}
$G_X(\theta) = 2(3-\theta)^{-1}$
\end{center}

(a) Suppose that $Y=X^2$. Calculate $P(Y=k)$ for $0\le k \le 10$ and the expectaion $E[Y]$\
\
$G_X(\theta) = 2(3-\theta)^{-1}$

$=\frac{2}{3}(1-\frac{\theta}{3})^{-1}$

$=\frac{2}{3}(1+\frac{\theta}{3}+(\frac{\theta}{3})^2+(\frac{\theta}{3})^3+....)$

$=\frac{2}{3}\theta^0+\frac{2}{3^2}\theta^1+\frac{2}{3^3}\theta^2+\frac{2}{3^4}\theta^3+...$\


We observe from the above equation that $P(X=0) = \frac{2}{3}, P(X=1)=\frac{2}{3^2} , P(X=2) = \frac{2}{3^3}$ ....\
\
$G_Y(\theta) = \sum_{k}^{\infty}\theta^kP(Y=k) = \sum_{k}^{\infty}\theta^kP(X^2=k)$\

$=\theta^0P(X^2=0)+\theta^1P(X^2=1)+...+\theta^4P(X^2=4)+....+\theta^9P(X^2=9)+\theta^{10}P(X^2=10)$\

As X can only take on positive integer values, the only terms in the above equation which are non-zero are those associated where $X^2 \in \{0,1,4,9\}$\

Therefore, $G_Y(\theta)=\theta^0P(X^2=0)+\theta^1P(X^2=1)+\theta^4P(X^2=4)+\theta^9P(X^2=9)$\
$G_Y(\theta)=\theta^0P(X=0)+\theta^1P(X=1)+\theta^4P(X=2)+\theta^9P(X=3)$\

$G_Y(\theta)=\frac{2}{3}\theta^0+\frac{2}{3^2}\theta^1+\frac{2}{3^3}\theta^4+\frac{2}{3^4}\theta^9$\ **Ans**

Now, to calculate $E[Y]$, we can evaluate $E[X^2]$, which is as follows.
$G''_X(\theta)|_{\theta=1} = E[X(X-1)] = E[X^2] - (E[X])$\
$\implies G''_X(\theta)|_{\theta=1} = (-2)\cdot\frac{2}{(3-\theta)^3}\cdot(-1) |_{\theta=1} = \frac{4}{2^3} = 0.5$\
$\implies E[X^2] - (E[X]) = 0.5$\

To calculate $E[X]$, we have $G'_X(\theta)|_{\theta=1} = E[X]$\
$\implies E[X] = \frac{2}{(3-\theta)^2}|_{\theta=1} = 0.5$

Therefore, going back to our earlier equation, we have
$\implies E[X^2] - (0.5) = 0.5$\
$\implies E[X^2] = E[Y] = 1$ **Ans**

(b) Suppose that Z is the random variable with probability generating function $G_Z(\theta) = G_X(G_X(\theta))$. Calculate $P(Z = 0), P(Z = 2)$, and $E[Z]$.\
\

$G_Z(\theta) = G_X(2(3-\theta)^{-1}) = 2(3 - \frac{2}{3-\theta})^{-1} = \frac{2(3-\theta)}{3(3-\theta)-2} = \frac{2(3 - \theta)}{7-3\theta}$\

$\implies G_Z(\theta) = \frac{2}{3}\frac{9-3\theta}{7-3\theta} = \frac{2}{3}(1+\frac{2}{7-3\theta}) = \frac{2}{3} + \frac{2}{3}\cdot2\cdot(7-3\theta)^{-1}$\

$\implies G_Z(\theta) = \frac{2}{3} + \frac{4}{3}\cdot(7(1-\frac{3}{7}\theta))^{-1} = \frac{2}{3} + \frac{4}{3}\cdot\frac{1}{7}(1 - \frac{3}{7}\theta)^{-1}$\

$\implies G_Z(\theta) = \frac{2}{3} + \frac{4}{21}\cdot(1 + \frac{3}{7}\theta + (\frac{3}{7}\theta)^2 +  (\frac{3}{7}\theta)^3 + .....)$\

$\implies G_Z(\theta) = \frac{2}{3} + \frac{4}{21} + \frac{4}{21}\cdot(\frac{3}{7}\theta + (\frac{3}{7}\theta)^2 +  (\frac{3}{7}\theta)^3 + .....)$\

$\implies G_Z(\theta) = \frac{18}{21} + \frac{4}{21}\cdot(\frac{3}{7}\theta + (\frac{3}{7}\theta)^2 +  (\frac{3}{7}\theta)^3 + .....)$\


From the above equation, we see that $P(Z=0)=\frac{18}{21}=0.8571$ **Ans**, $P(Z=2)=\frac{4}{21}\cdot\frac{3^2}{7^2}=0.0349$ **Ans**\

Since $G_Z(\theta)=G^2_X(\theta)$, we know that $E[Z]=\mu_X^2$, where $\mu_X = E[X]$. 

To find this, we evaluate $G'_X(\theta)|_{\theta=1} = \frac{d}{d\theta}(2(3-\theta)^{-1}) = 2 \frac{d}{d\theta}\frac{1}{3-\theta} = 2\cdot(-1)\cdot\frac{1}{(3-\theta)^2}\cdot(-1) = \frac{2}{(3-\theta)^2}$\

Putting the value of $\theta=1$, we have, $E[X]=\frac{2}{(3-1)^2}=\frac{2}{2^2}=0.5$
Therefore, $E[Z]=0.5^2=0.25$ **Ans**

(c) $W = X_1 + X_2 +X_3$, where $X_1,X_2,X_3$ are independent

Therefore, $G_W(\theta)=\{2(3-\theta)^{-1}\}^3 = 2^3(3-\theta)^{-3} = 8(3-\theta)^{-3} = \frac{8}{27}(1-\frac{\theta}{3})^{-3}$

Using binomial expansion for negative coefficients, we have\
$G_W(\theta)=\frac{8}{27}(1+(-3)(-\frac{\theta}{3}) + \frac{(-3)(-3-1)}{2!}\frac{\theta^2}{3^2}+...)$\

$G_W(\theta)=\frac{8}{27}(1+\theta + \frac{2}{3}\theta^2+...)$\

From the above equation, it can be observed that\
$P(W=0)=\frac{8}{27}=0.2962$ **Ans** , $P(W=2)=\frac{8*2}{27*3}=0.1975$ **Ans**

For $E[W]$, we have $G'_W(\theta)|_{\theta=1}=2^3(-3)(3-\theta)^{-4}(-1)|_{\theta=1}=\frac{24}{(3-1)^4} = \frac{24}{16}=1.5$ **Ans**

**2** Suppose that X is a discrete random variable with probability generating function:
$G_X(\theta) = \frac{1-2\lambda}{1-\lambda\theta-\lambda\theta^2}$, where $\lambda>0$ is a constant. Consider a branching process $\{S_n\}_{n\ge0}$ denoting the number of
individuals at stage n (notation as in lectures), with $S_0 = 1$. Assume the number of offspring
$X$ arising from any individual in any generation has the probability distribution governed by
$G_X(\theta)$, and that all individuals evolve independently of all others.

(a) Write down the range of values of $\lambda$ for which $G_X$ is a valid probability generating function.

$G_X(\theta) = (1-2\lambda)(1-\lambda\theta^2-\lambda\theta)^{-1}=(1-2\lambda)(1-\lambda\theta(\theta+1))^{-1}$

Expanding, we have

$G_X(\theta) = (1-2\lambda)[1+\lambda\theta(\theta+1) + (\lambda\theta(\theta+1))^2+(\lambda\theta(\theta+1))^3+....]$

It is clear from the above equation that $P(X=0)$ or the coefficient of $\theta^0$ is $(1-2\lambda)$

For $G_X(\theta)$ to be a valid PGF, $0<(1-2\lambda)\le1 \implies 0 \le \lambda <\frac{1}{2} \implies \lambda \in [0,\frac{1}{2})$ **Ans**

(b) For $\lambda=\frac{1}{4}$, $G_X(\theta)=\frac{1-2\lambda}{1-\lambda\theta-\lambda\theta^2}=\frac{1-2\cdot\frac{1}{4}}{1-\frac{1}{4}\theta - \frac{1}{4}\theta^2}=\frac{2}{4-\theta-\theta^2}$

$\mu_X=G_X'(\theta)|_{\theta=1}=\frac{2}{(4-\theta-\theta^2)^2}\cdot(-1)\cdot(-1-2\theta)=\frac{2}{(4-\theta-\theta^2)^2}\cdot(1+2\theta)$

Putting the value of $\theta$,
$\mu_X = \frac{2}{(4-1-1^2)^2}(1+2\cdot1)=\frac{6}{4}=1.5$

$E[S_n]=\mu_X^n=(1.5)^n$ **Ans**

$G_{S1}(\theta) = G_X^{1}(\theta) = G_X(\theta) = (1-2\lambda)[1+\lambda\theta(\theta+1) + (\lambda\theta(\theta+1))^2+(\lambda\theta(\theta+1))^3+...]$

$\implies G_{S1}(\theta) = (1-2\lambda)[1 + \lambda\theta(\theta+1) + \lambda^2\theta^2(\theta+1)^2+...]$

$\implies G_{S1}(\theta) = (1-2\lambda)[1 + \lambda\theta^2 +  \lambda\theta + \lambda^2\theta^2(\theta^2+2\theta+1)+...]$
 
$\implies G_{S1}(\theta) = (1-2\lambda)[1 + \lambda\theta^2 +  \lambda\theta + \lambda^2\theta^2(\theta^2+2\theta+1)+...]$
 
$\implies G_{S1}(\theta) = (1-2\lambda)[1 + \lambda\theta^2 +  \lambda\theta + \lambda^2\theta^4 + 2\lambda^2\theta^3 + \lambda^2\theta^2+...]$

Here, $P(S_1=2)$ is the coefficient of $\theta^2$, which is $(1-2\lambda)(\lambda^2+\lambda) = (1-\frac{2}{4})(\frac{1}{16}+\frac{1}{4}) = 0.5*0.3125 = 0.15625$ **Ans**

Probability of extinction is given by the smallest root of the equation $G_X(e)=e$

$G_X(e) = e \implies \frac{1-2\lambda}{1-\lambda e-\lambda e^2} = e$
$\implies 1-2\lambda= e\cdot(1-\lambda e-\lambda e^2)$
$\implies 1-2\lambda = e - \lambda e^2 - \lambda e^3$
$\implies \lambda e^3 + \lambda e^2 - e - 2\lambda + 1=0$

We know that ($e$-1) must always be a factor as the probability of extinction is upper-bounded by 1.

Factorizing,

$(e-1)(\lambda e^2 + 2\lambda e - 1  + 2 \lambda)=0$

Now, either $e = 1$ or $(\lambda e^2 + 2\lambda e - 1  + 2 \lambda)=0$

Using the Sridharacharya formula for solving quadratics, we have

$e = \frac{-2\lambda \pm \sqrt{4\lambda^2-4\lambda(2\lambda-1)}}{2\lambda}$

$e = \frac{-2\lambda \pm \sqrt{4\lambda^2-4\lambda^2\cdot\frac{1}{\lambda}(2\lambda-1)}}{2\lambda}$

$e = \frac{-2\lambda \pm \sqrt{4\lambda^2(1-\frac{1}{\lambda}(2\lambda-1))}}{2\lambda}$

$e = \frac{-2\lambda \pm 2\lambda\sqrt{(1-\frac{1}{\lambda}(2\lambda-1))}}{2\lambda}$

$e = -1 \pm \sqrt{(1-\frac{1}{\lambda}(2\lambda-1))}$

$e = -1 \pm \sqrt{(1-2+\frac{1}{\lambda})}$

$e = -1 \pm \sqrt{(\frac{1}{\lambda}-1)}$

Using $\lambda=\frac{1}{4}$, we have $e = -1 \pm \sqrt3$. Since the negative solution is nonsensical, $e = -1 + \sqrt3 = 0.732$ **Ans**

(c) Compute the range of values of $\lambda$ for which the population is guaranteed to become extinct

To achieve this, we need $\mu_X < 1$

$\mu_X = G_x'(\theta)|_{\theta=1} = \frac{1-2\lambda}{(1-\lambda \theta - \lambda \theta^2)^2}\cdot(-1)\cdot(-\lambda - 2\lambda\theta) < 1$

Substituting $\theta=1$, we have $\mu_X = \frac{1-2\lambda}{(1-\lambda - \lambda )^2}(\lambda + 2\lambda) = \frac{1-2\lambda}{(1-2\lambda)^2}3\lambda < 1$

$(1-2\lambda)(3\lambda)<(1-2\lambda)^2$

$(1-2\lambda)(3\lambda)-(1-2\lambda)^2<0$

$(1-2\lambda)(3\lambda-(1-2\lambda))<0$

$(1-2\lambda)(5\lambda-1)<0$

$(2\lambda-1)(5\lambda-1)>0$

$(\lambda-\frac{1}{2})(\lambda-\frac{1}{5})>0$

This inequality yields $\lambda \in  (-\infty,\frac{1}{5})\cap(\frac{1}{2},\infty)$
Combining this with the earlier bounded region for $\lambda$ where ($\lambda \in [0,\frac{1}{2})$) for $G_X(\theta)$ to be a valid PGF, we have $\lambda \in [0,\frac{1}{5})$ **Ans**

(d) For ultimate extinction as a function  of $\lambda$, we refer to part (b). It is $-1+\sqrt{\frac{1}{\lambda}-1}$ **Ans**

**3** At the University of Exeter, the frequency of cars passing the Harrison Building are recorded.
It is noticed that Mercedes, BMWs and Ferraris pass by at a combined rate of 8 cars per hour.
Out of these cars, it is noticed that 40% are Mercedes, 50% are BMWs and the remaining 10%
are Ferraris. You may assume that all passing car types are independent Poisson processes.

(a) Find the probability that the next car observed is a Mercedes.

Since all of the passing cars follow independent Poisson processes, P(next Mercedes) = 0.4 **Ans**

(b) Given that the last car observed was a BMW, find the probability that the next car to be
observed passing the Harrison Building is a Ferrari.

We have independence of events in the future from events in the past. Therefore P(next car Ferrari) = $0.1$ **Ans**

(c) Calculate the probability that exactly 2 BMWS pass by the Harrison Building in 30 minutes.

The combined rate of passing is known to be $\lambda_{combined}=8/hr$. Since BMWs account for 50% of the volume, $\lambda_{BMW}=0.5*8=4/hr$

Recall that formula for Poisson process is $P((N=t)=k)=\frac{(\lambda t)^k e^{-\lambda t}}{k!}$
Hence, $P(N(t=1/2)=2) = \frac{(4\cdot\frac{1}{2})^2\cdot e^{-2}}{2!} = 2e^{-2} = 0.2706$ **Ans**

(d) Calculate the probability that at least three cars (of any of those three types) pass by the
Harrison building in 45 minutes.

$\lambda=8$ and $t=3/4$, which implies $\lambda t=8*\frac{3}{4} = 6$

$P(N(t=\frac{3}{4})\ge3) = 1-P(N(t=\frac{3}{4})<3) = 1 - P(N(t=\frac{3}{4})\le2) = 1 - \sum_{k=0}^{2}P(N(t=\frac{3}{4})=k)$

$=1 - \frac{(6)^2e^{-6}}{2!} - \frac{(6)^1e^{-6}}{1!} - \frac{(6)^0e^{-6}}{0!}$
$=1-0.04461 - 0.0148 - 0.00247 = 0.9380312$ **Ans**

(e) Calculate the probability that the time between seeing the first Ferrari and the fifth Ferrari
exceeds 2 hours.

This can be interpreted as the time taken for 4 sightings of Ferraris is above 2 hours. Another way to look at it is that in 2 hours, 3 or fewer Ferraris are observed to pass. We know that the combined rate $\lambda_{combined}=8$. Since Ferraris account for 10% of that, we can say the rate of Ferraris passing is $\lambda_F=8*0.1=0.8$ and $t=2$. Therfore, $\lambda_F\cdot t=0.8*2=1.6$

$P(S_4>2) = P(N(t=2)\le3) = \sum_{k=0}^{3}P(N(t=2)=k) = \sum_{k=0}^{3}\frac{(\lambda_F\cdot t)^ke^{(-\lambda_F\cdot t)}}{k!}$

$= \sum_{k=0}^{3}\frac{1.6^ke^{-1.6}}{k!} = 0.9212$ **Ans**

(f) What is the probability that it takes between 1 and 2 hours to see the 3rd BMW pass by the Harrison Building?

From (c), we know that rate of BMWs passing Harrison building is $4/hr$. We need to calculate $P(1 \le S_3 \le 2)$ where $S_3$ is the time taken to see the 3rd BMW pass.

$P(1 \le S_3 \le 2) = P(S_3 \le 2) - P(S_3 \le 1) = P(N(t=2)\ge 3) - P(N(t=1) \ge 3)$

$= 1 - P(N(t=2)<3) - [1 - P(N(t=1) < 3)] = P(N(t=1)<3) - P(N(t=2)<3)$

$= P(N(t=1) \le 2)-P(N(t=2) \le2 )$

$= \sum_{k=0}^{2}P(N(t=1) = k)-P(N(t=2) = k )$

$= \sum_{k=0}^{2} \frac{(4*1)^ke^{-4*1}}{k!} - \frac{(4*2)^ke^{-4*2}}{k!}$

$= \sum_{k=0}^{2} \frac{(4*1)^ke^{-4}}{k!}(1 - 2^ke^{-4})$

$=\frac{4^0e^{-4}}{0!}(1-2^0e^{-4}) + \frac{4^1e^{-4}}{1!}(1-2^1e^{-4}) + \frac{4^2e^{-4}}{2!}(1-2^2e^{-4}) = 0.2243$ **Ans**
\
```{r}
k <- c(0,1,2)
sum(4^k/factorial(k)*exp(-4)*(1 - 2^k*exp(-4)))
```

**4** Consider the random variable X with a zero-truncated Poisson distribution given by

$P(X=0)=0,$ and $P(X=k)=c_{\lambda}\frac{\lambda^ke^{-\lambda}}{k!}, \ \lambda > 0, k \ge 1$

(a) Find the constant $c_{\lambda}$ in terms of $\lambda$

For this to be a valid probability distribution, $\sum_{k=0}^{\infty}P(X=k)=1$

$\implies P(X=0) + \sum_{k=1}^{\infty}P(X=k)=1$

$\implies 0 + \sum_{k=1}^{\infty}P(X=k)=1$

$\implies \sum_{k=1}^{\infty}c_{\lambda}\frac{\lambda^ke^{-\lambda}}{k!}=1$

$\frac{c_{\lambda}}{e^{\lambda}}\sum_{k=1}^{\infty}\frac{\lambda^k}{k!} = 1$

$\sum_{k=1}^{\infty}\frac{\lambda^k}{k!} = \frac{e^{\lambda}}{c_{\lambda}}$

$1+ \sum_{k=1}^{\infty}\frac{\lambda^k}{k!} =1+ \frac{e^{\lambda}}{c_{\lambda}}$

$e^{\lambda} = \frac{e^{\lambda}}{c_{\lambda}}+1$

$\frac{e^{\lambda}-1}{e^{\lambda}} = \frac{1}{c_{\lambda}}$

$c_{\lambda} = \frac{e^{\lambda}}{e^{\lambda}-1}$

(b) Derive the probability generating function for X and use it to find E[X] and Var(X)

$G_X(\theta) = \sum_{k=0}^{\infty}\theta^kP(X=k) =\theta^0P(X=0) + \sum_{k=1}^{\infty}\theta^kP(X=k)$

$\implies G_X(\theta) = \sum_{k=1}^{\infty}\theta^kP(X=k) = \sum_{k=1}^{\infty}\theta^k\frac{c_{\lambda}\lambda^ke^{-\lambda}}{k!}$

$\implies G_X(\theta) = c_{\lambda}e^{-\lambda}\sum_{k=1}^{\infty}\frac{(\lambda\theta)^k}{k!} = c_{\lambda}e^{-\lambda}[\sum_{k=0}^{\infty}\frac{(\lambda\theta)^k}{k!}-1] = c_{\lambda}e^{-\lambda}(e^{\lambda\theta}-1)$

$\implies G_X(\theta) =   \frac{e^{\lambda}}{e^{\lambda}-1}e^{-\lambda}(e^{\lambda\theta}-1)= \frac{e^{\lambda\theta}-1}{e^{\lambda}-1}$ **Ans**

$E[X] = G_X'(\theta)|_{\theta=1} = \frac{1}{e^{\lambda}-1}\lambda e^{\lambda\theta}|_{\theta=1} = \frac{\lambda e^{\lambda\theta}}{e^{\lambda}-1}|_{\theta=1} = \frac{\lambda e^{\lambda}}{e^{\lambda}-1}$

For variance, $Var[X] = E[X^2]-(E[X])^2$

We know that $G_X''(\theta)|_{\theta=1} =E[X(X-1)] = E[X^2] - E[X]$

$\implies E[X^2] - E[X] = \frac{d^2}{d\theta^2}(G_X(\theta))|_{\theta=1}=\frac{d}{d\theta}(\frac{\lambda e^{\lambda\theta}}{e^{\lambda}-1}) = \frac{\lambda^2 e^{\lambda\theta}}{e^{\lambda}-1} = \frac{\lambda^2 e^{\lambda}}{e^{\lambda}-1}$

$\implies E[X^2] = \frac{\lambda^2 e^{\lambda}}{e^{\lambda}-1} + E[X]$

$\implies E[X^2] = \frac{\lambda^2 e^{\lambda}}{e^{\lambda}-1} + \frac{\lambda e^{\lambda}}{e^{\lambda}-1}$

$\implies Var[X] = E[X^2] - (E[X])^2 = \frac{\lambda e^{\lambda}}{e^{\lambda}-1}(\lambda+1) - (\frac{\lambda e^{\lambda}}{e^{\lambda}-1})^2$

$\implies Var[X] = \frac{\lambda e^{\lambda}}{e^{\lambda}-1}(\lambda+1 - \frac{\lambda e^{\lambda}}{e^{\lambda}-1})$

$\implies Var[X] = \frac{\lambda e^{\lambda}}{(e^{\lambda}-1)^2}(e^{\lambda}-(\lambda+1))$ **Ans**

**(c)** Suppose that X and Y are two independent random variables each having a zero-truncated
Poisson distribution with parameters $\lambda$ and $\mu$, respectively. Find the probability generating function for Z where Z = X + Y . Hence show that for $k \ge 2$:
$P(Z = k) = C(\lambda, \mu, k)[(\lambda + \mu)^k - \lambda^k - \mu^k]$

$G_X(\theta) = c_{\lambda}e^{-\lambda}[e^{\lambda\theta}-1]$\

$G_Y(\theta) = c_{\mu}e^{-\mu}[e^{\mu\theta}-1]$\ 

$G_Z(\theta) = G_X(\theta)G_Y(\theta) = \frac{c_{\lambda}c_{\mu}}{e^{(\lambda+\mu)}}[e^{\lambda\theta}-1][e^{\mu\theta}-1]$

$\implies G_Z(\theta) = \frac{e^{\lambda}}{e^{\lambda}-1}\frac{1}{e^{\lambda}}\frac{e^{\mu}}{e^{\mu}-1}\frac{1}{e^{\mu}}[e^{\lambda\theta}-1][e^{\mu\theta}-1]$

$\implies G_Z(\theta) = \frac{1}{e^{\lambda}-1}\frac{1}{e^{\mu}-1}[e^{\lambda\theta}-1][e^{\mu\theta}-1]$

$\implies G_Z(\theta) = \frac{e^{(\lambda+\mu)\theta} - e^{\lambda\theta} - e^{\mu\theta} + 1}{(e^{\lambda} - 1)(e^{\mu} - 1)}$

$\implies G_Z(\theta) = \frac{1}{(e^{\lambda} - 1)(e^{\mu} - 1)}[1 + \frac{(\lambda+\mu)\theta}{1!} + \frac{(\lambda+\mu)^2\theta^2}{2!} \frac{(\lambda+\mu)^3\theta^3}{3!} +  .... - (1 + \frac{(\lambda)\theta}{1!} + \frac{(\lambda)^2\theta^2}{2!} \frac{(\lambda)^3\theta^3}{3!} + ....) - (1 + \frac{(\mu)\theta}{1!} + \frac{(\mu)^2\theta^2}{2!} \frac{(\mu)^3\theta^3}{3!} + ....) + 1]$

$\implies G_Z(\theta) = \frac{1}{(e^{\lambda} - 1)(e^{\mu} - 1)}[\theta\frac{(\lambda + \mu) - \lambda - \mu}{1!} + \theta^2\frac{(\lambda + \mu)^2 - \lambda^2 - \mu^2}{2!} + \theta^3\frac{(\lambda + \mu)^3 - \lambda^3 - \mu^3}{3!} + ......]$


$\implies G_Z(\theta) = \frac{1}{(e^{\lambda} - 1)(e^{\mu} - 1)}[\theta^2\frac{(\lambda + \mu)^2 - \lambda^2 - \mu^2}{2!} + \theta^3\frac{(\lambda + \mu)^3 - \lambda^3 - \mu^3}{3!} + ......]$

Following the above pattern, we can say that $P(Z=k)$ is the coefficient of $\theta^k$. Therefore,

$P(Z=k) = \frac{1}{k!(e^{\lambda} - 1)(e^{\mu} - 1)}[(\lambda + \mu)^k - \lambda^k - \mu^k]$

This can be written as required in the question. 

$P(Z=k) = C(\lambda,\mu, k)[(\lambda + \mu)^k - \lambda^k - \mu^k]$, where $C(\lambda,\mu, k) = \frac{1}{k!(e^{\lambda} - 1)(e^{\mu} - 1)}$ **Ans**
